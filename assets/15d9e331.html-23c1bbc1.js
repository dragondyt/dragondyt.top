import{_ as t,o as d,c as a,e as r}from"./app-5de3c553.js";const e={},p=r('<h1 id="下载" tabindex="-1"><a class="header-anchor" href="#下载" aria-hidden="true">#</a> 下载</h1><p><a href="https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz" target="_blank" rel="noopener noreferrer">下载hadoop</a></p><p><img src="https://img-blog.csdnimg.cn/20210423100922519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM2OTM3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="配置环境变量" tabindex="-1"><a class="header-anchor" href="#配置环境变量" aria-hidden="true">#</a> 配置环境变量</h1><p><img src="https://img-blog.csdnimg.cn/20210423112247297.png" alt="在这里插入图片描述"><br> path路径<br><img src="https://img-blog.csdnimg.cn/20210423112306293.png" alt="在这里插入图片描述"></p><h1 id="配置文件" tabindex="-1"><a class="header-anchor" href="#配置文件" aria-hidden="true">#</a> 配置文件</h1><p><code>hadoop-3.2.2\\etc\\hadoop\\core-site.xml</code></p><figure class="highlight xml"><figcaption data-lang="xml"></figcaption><div class="code-container"><table><tr><td data-num="1"></td><td><pre>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</pre></td></tr><tr><td data-num="2"></td><td><pre>&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</pre></td></tr><tr><td data-num="3"></td><td><pre>&lt;configuration&gt;</pre></td></tr><tr><td data-num="4"></td><td><pre> &lt;property&gt;</pre></td></tr><tr><td data-num="5"></td><td><pre>        &lt;name&gt;fs.defaultFS&lt;/name&gt;</pre></td></tr><tr><td data-num="6"></td><td><pre>        &lt;value&gt;hdfs://localhost:9000/&lt;/value&gt;</pre></td></tr><tr><td data-num="7"></td><td><pre> &lt;/property&gt;</pre></td></tr><tr><td data-num="8"></td><td><pre> &lt;!-- 当前用户全设置成root --&gt;</pre></td></tr><tr><td data-num="9"></td><td><pre>&lt;property&gt;</pre></td></tr><tr><td data-num="10"></td><td><pre>&lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</pre></td></tr><tr><td data-num="11"></td><td><pre>&lt;value&gt;root&lt;/value&gt;</pre></td></tr><tr><td data-num="12"></td><td><pre>&lt;/property&gt;</pre></td></tr><tr><td data-num="13"></td><td><pre></pre></td></tr><tr><td data-num="14"></td><td><pre>&lt;!-- 不开启权限检查 --&gt;</pre></td></tr><tr><td data-num="15"></td><td><pre>&lt;property&gt;</pre></td></tr><tr><td data-num="16"></td><td><pre>&lt;name&gt;dfs.permissions.enabled&lt;/name&gt;</pre></td></tr><tr><td data-num="17"></td><td><pre>&lt;value&gt;false&lt;/value&gt;</pre></td></tr><tr><td data-num="18"></td><td><pre>&lt;/property&gt;</pre></td></tr><tr><td data-num="19"></td><td><pre></pre></td></tr><tr><td data-num="20"></td><td><pre>&lt;/configuration&gt;</pre></td></tr><tr><td data-num="21"></td><td><pre></pre></td></tr></table><div class="operation"><span class="breakline-btn"><i class="ic i-align-left"></i></span><span class="copy-btn"><i class="ic i-clipboard"></i></span><span class="fullscreen-btn"><i class="ic i-expand"></i></span></div></div></figure><p><code>hadoop-3.2.2\\etc\\hadoop\\hdfs-site.xml</code></p><figure class="highlight xml"><figcaption data-lang="xml"></figcaption><div class="code-container"><table><tr><td data-num="1"></td><td><pre>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</pre></td></tr><tr><td data-num="2"></td><td><pre>&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</pre></td></tr><tr><td data-num="3"></td><td><pre>&lt;configuration&gt;</pre></td></tr><tr><td data-num="4"></td><td><pre>    &lt;property&gt;</pre></td></tr><tr><td data-num="5"></td><td><pre>        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</pre></td></tr><tr><td data-num="6"></td><td><pre>        &lt;value&gt;/d:/DevTools/hadoop-3.2.2/data/namenode&lt;/value&gt;</pre></td></tr><tr><td data-num="7"></td><td><pre>        &lt;description&gt;NameNode directory for namespace and transaction logs storage.&lt;/description&gt;</pre></td></tr><tr><td data-num="8"></td><td><pre>    &lt;/property&gt;</pre></td></tr><tr><td data-num="9"></td><td><pre>    &lt;property&gt;</pre></td></tr><tr><td data-num="10"></td><td><pre>        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</pre></td></tr><tr><td data-num="11"></td><td><pre>        &lt;value&gt;/d:/DevTools/hadoop-3.2.2/data/datanode&lt;/value&gt;</pre></td></tr><tr><td data-num="12"></td><td><pre>        &lt;description&gt;DataNode directory&lt;/description&gt;</pre></td></tr><tr><td data-num="13"></td><td><pre>    &lt;/property&gt;</pre></td></tr><tr><td data-num="14"></td><td><pre>    &lt;property&gt;</pre></td></tr><tr><td data-num="15"></td><td><pre>        &lt;name&gt;dfs.replication&lt;/name&gt;</pre></td></tr><tr><td data-num="16"></td><td><pre>        &lt;value&gt;2&lt;/value&gt;</pre></td></tr><tr><td data-num="17"></td><td><pre>    &lt;/property&gt;</pre></td></tr><tr><td data-num="18"></td><td><pre>&lt;/configuration&gt;</pre></td></tr><tr><td data-num="19"></td><td><pre></pre></td></tr></table><div class="operation"><span class="breakline-btn"><i class="ic i-align-left"></i></span><span class="copy-btn"><i class="ic i-clipboard"></i></span><span class="fullscreen-btn"><i class="ic i-expand"></i></span></div></div></figure><p><code>hadoop-3.2.2\\etc\\hadoop\\mapred-site.xml</code></p><figure class="highlight xml"><figcaption data-lang="xml"></figcaption><div class="code-container"><table><tr><td data-num="1"></td><td><pre>&lt;?xml version=&quot;1.0&quot;?&gt;</pre></td></tr><tr><td data-num="2"></td><td><pre>&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</pre></td></tr><tr><td data-num="3"></td><td><pre>&lt;configuration&gt;</pre></td></tr><tr><td data-num="4"></td><td><pre>    &lt;property&gt;</pre></td></tr><tr><td data-num="5"></td><td><pre>        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</pre></td></tr><tr><td data-num="6"></td><td><pre>        &lt;value&gt;yarn&lt;/value&gt;</pre></td></tr><tr><td data-num="7"></td><td><pre>    &lt;/property&gt;</pre></td></tr><tr><td data-num="8"></td><td><pre>&lt;/configuration&gt;</pre></td></tr><tr><td data-num="9"></td><td><pre></pre></td></tr></table><div class="operation"><span class="breakline-btn"><i class="ic i-align-left"></i></span><span class="copy-btn"><i class="ic i-clipboard"></i></span><span class="fullscreen-btn"><i class="ic i-expand"></i></span></div></div></figure><p><code>hadoop-3.2.2\\etc\\hadoop\\yarn-site.xml</code></p><figure class="highlight xml"><figcaption data-lang="xml"></figcaption><div class="code-container"><table><tr><td data-num="1"></td><td><pre>&lt;?xml version=&quot;1.0&quot;?&gt;</pre></td></tr><tr><td data-num="2"></td><td><pre>&lt;configuration&gt;</pre></td></tr><tr><td data-num="3"></td><td><pre>&lt;property&gt;</pre></td></tr><tr><td data-num="4"></td><td><pre>        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</pre></td></tr><tr><td data-num="5"></td><td><pre>        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</pre></td></tr><tr><td data-num="6"></td><td><pre>    &lt;/property&gt;</pre></td></tr><tr><td data-num="7"></td><td><pre>    &lt;property&gt;</pre></td></tr><tr><td data-num="8"></td><td><pre>        &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;</pre></td></tr><tr><td data-num="9"></td><td><pre>        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</pre></td></tr><tr><td data-num="10"></td><td><pre>    &lt;/property&gt;</pre></td></tr><tr><td data-num="11"></td><td><pre>    &lt;property&gt;</pre></td></tr><tr><td data-num="12"></td><td><pre>        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</pre></td></tr><tr><td data-num="13"></td><td><pre>        &lt;value&gt;localhost&lt;/value&gt;</pre></td></tr><tr><td data-num="14"></td><td><pre>    &lt;/property&gt;</pre></td></tr><tr><td data-num="15"></td><td><pre>&lt;/configuration&gt;</pre></td></tr></table><div class="operation"><span class="breakline-btn"><i class="ic i-align-left"></i></span><span class="copy-btn"><i class="ic i-clipboard"></i></span><span class="fullscreen-btn"><i class="ic i-expand"></i></span></div></div></figure><h1 id="缺失文件" tabindex="-1"><a class="header-anchor" href="#缺失文件" aria-hidden="true">#</a> 缺失文件</h1><p><img src="https://img-blog.csdnimg.cn/20210423101547514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM2OTM3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><a href="https://github.com/cdarlint/winutils/blob/master/hadoop-3.2.1" target="_blank" rel="noopener noreferrer">github下载</a></p><h1 id="遇到的问题" tabindex="-1"><a class="header-anchor" href="#遇到的问题" aria-hidden="true">#</a> 遇到的问题</h1><p><img src="https://img-blog.csdnimg.cn/20210423110450309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM2OTM3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><blockquote><p>解决方案:安装<a href="https://blog.csdn.net/vbcom/article/details/7245186" target="_blank" rel="noopener noreferrer">DirectX修复工具增强版</a>修复系统组件缺失,在C://Windows/System32目录下放置hadoop.dll</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20210423110554561.png" alt="在这里插入图片描述"></p><blockquote><p>ps: 本来打算放弃的,win下的坑太多了,一顿瞎搞后终于成功了,还有点小问题</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20210423122205257.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM2OTM3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="节点未格式化" tabindex="-1"><a class="header-anchor" href="#节点未格式化" aria-hidden="true">#</a> 节点未格式化</h2><p>执行</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><div class="code-container"><table><tr><td data-num="1"></td><td><pre>hdfs namenode  -format</pre></td></tr></table><div class="operation"><span class="breakline-btn"><i class="ic i-align-left"></i></span><span class="copy-btn"><i class="ic i-clipboard"></i></span><span class="fullscreen-btn"><i class="ic i-expand"></i></span></div></div></figure><p>输入Y<br><img src="https://img-blog.csdnimg.cn/20210423122919553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM2OTM3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="启动" tabindex="-1"><a class="header-anchor" href="#启动" aria-hidden="true">#</a> 启动</h1><p>执行命令或双击hadoop-3.2.2\\sbin下的start-all.cmd</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><div class="code-container"><table><tr><td data-num="1"></td><td><pre>./start-all.cmd</pre></td></tr></table><div class="operation"><span class="breakline-btn"><i class="ic i-align-left"></i></span><span class="copy-btn"><i class="ic i-clipboard"></i></span><span class="fullscreen-btn"><i class="ic i-expand"></i></span></div></div></figure><h2 id="访问8088" tabindex="-1"><a class="header-anchor" href="#访问8088" aria-hidden="true">#</a> 访问8088</h2><p><img src="https://img-blog.csdnimg.cn/20210423123512815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM2OTM3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="访问9000" tabindex="-1"><a class="header-anchor" href="#访问9000" aria-hidden="true">#</a> 访问9000</h2><p><code>It looks like you are making an HTTP request to a Hadoop IPC port. This is not the correct port for the web interface on this daemon.</code></p><h2 id="访问9870-hadoop新版本默认" tabindex="-1"><a class="header-anchor" href="#访问9870-hadoop新版本默认" aria-hidden="true">#</a> 访问9870(hadoop新版本默认)</h2><p><img src="https://img-blog.csdnimg.cn/20210423123926928.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM2OTM3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="关闭" tabindex="-1"><a class="header-anchor" href="#关闭" aria-hidden="true">#</a> 关闭</h1><p>执行命令或双击hadoop-3.2.2\\sbin下的stop-all.cmd</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><div class="code-container"><table><tr><td data-num="1"></td><td><pre>./stop-all.cmd</pre></td></tr></table><div class="operation"><span class="breakline-btn"><i class="ic i-align-left"></i></span><span class="copy-btn"><i class="ic i-clipboard"></i></span><span class="fullscreen-btn"><i class="ic i-expand"></i></span></div></div></figure><h1 id="开发时修改权限" tabindex="-1"><a class="header-anchor" href="#开发时修改权限" aria-hidden="true">#</a> 开发时修改权限</h1><p><code>core-site.xml</code></p><figure class="highlight xml"><figcaption data-lang="xml"></figcaption><div class="code-container"><table><tr><td data-num="1"></td><td><pre>&lt;!-- 当前用户全设置成root --&gt;</pre></td></tr><tr><td data-num="2"></td><td><pre>&lt;property&gt;</pre></td></tr><tr><td data-num="3"></td><td><pre>&lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</pre></td></tr><tr><td data-num="4"></td><td><pre>&lt;value&gt;root&lt;/value&gt;</pre></td></tr><tr><td data-num="5"></td><td><pre>&lt;/property&gt;</pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre>&lt;!-- 不开启权限检查 --&gt;</pre></td></tr><tr><td data-num="8"></td><td><pre>&lt;property&gt;</pre></td></tr><tr><td data-num="9"></td><td><pre>&lt;name&gt;dfs.permissions.enabled&lt;/name&gt;</pre></td></tr><tr><td data-num="10"></td><td><pre>&lt;value&gt;false&lt;/value&gt;</pre></td></tr><tr><td data-num="11"></td><td><pre>&lt;/property&gt;</pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr></table><div class="operation"><span class="breakline-btn"><i class="ic i-align-left"></i></span><span class="copy-btn"><i class="ic i-clipboard"></i></span><span class="fullscreen-btn"><i class="ic i-expand"></i></span></div></div></figure><h1 id="测试" tabindex="-1"><a class="header-anchor" href="#测试" aria-hidden="true">#</a> 测试</h1><h2 id="环境准备" tabindex="-1"><a class="header-anchor" href="#环境准备" aria-hidden="true">#</a> 环境准备</h2><ol><li>使用图形界面添加等会用的的目录</li><li>上传测试文件word,内容为</li></ol><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><div class="code-container"><table><tr><td data-num="1"></td><td><pre>my name is abc, I can read abc.</pre></td></tr><tr><td data-num="2"></td><td><pre>this is a demo for mapreduce</pre></td></tr><tr><td data-num="3"></td><td><pre>I&#39;m learning hadoop</pre></td></tr><tr><td data-num="4"></td><td><pre>hadoop and mapreduce</pre></td></tr></table><div class="operation"><span class="breakline-btn"><i class="ic i-align-left"></i></span><span class="copy-btn"><i class="ic i-clipboard"></i></span><span class="fullscreen-btn"><i class="ic i-expand"></i></span></div></div></figure><p><img src="https://img-blog.csdnimg.cn/20210423142128838.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM2OTM3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="代码编写" tabindex="-1"><a class="header-anchor" href="#代码编写" aria-hidden="true">#</a> 代码编写</h2><p><code>pom.xml</code></p><figure class="highlight xml"><figcaption data-lang="xml"></figcaption><div class="code-container"><table><tr><td data-num="1"></td><td><pre>...</pre></td></tr><tr><td data-num="2"></td><td><pre>        &lt;!-- hadoop 依赖 --&gt;</pre></td></tr><tr><td data-num="3"></td><td><pre>        &lt;dependency&gt;</pre></td></tr><tr><td data-num="4"></td><td><pre>            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</pre></td></tr><tr><td data-num="5"></td><td><pre>            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</pre></td></tr><tr><td data-num="6"></td><td><pre>            &lt;version&gt;2.10.0&lt;/version&gt;</pre></td></tr><tr><td data-num="7"></td><td><pre>        &lt;/dependency&gt;</pre></td></tr><tr><td data-num="8"></td><td><pre>        &lt;dependency&gt;</pre></td></tr><tr><td data-num="9"></td><td><pre>            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</pre></td></tr><tr><td data-num="10"></td><td><pre>            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</pre></td></tr><tr><td data-num="11"></td><td><pre>            &lt;version&gt;2.10.0&lt;/version&gt;</pre></td></tr><tr><td data-num="12"></td><td><pre>        &lt;/dependency&gt;</pre></td></tr><tr><td data-num="13"></td><td><pre></pre></td></tr><tr><td data-num="14"></td><td><pre>        &lt;dependency&gt;</pre></td></tr><tr><td data-num="15"></td><td><pre>            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</pre></td></tr><tr><td data-num="16"></td><td><pre>            &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt;</pre></td></tr><tr><td data-num="17"></td><td><pre>            &lt;version&gt;2.10.0&lt;/version&gt;</pre></td></tr><tr><td data-num="18"></td><td><pre>        &lt;/dependency&gt;</pre></td></tr><tr><td data-num="19"></td><td><pre>        &lt;dependency&gt;</pre></td></tr><tr><td data-num="20"></td><td><pre>            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</pre></td></tr><tr><td data-num="21"></td><td><pre>            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</pre></td></tr><tr><td data-num="22"></td><td><pre>            &lt;version&gt;2.10.0&lt;/version&gt;</pre></td></tr><tr><td data-num="23"></td><td><pre>        &lt;/dependency&gt;</pre></td></tr><tr><td data-num="24"></td><td><pre>...</pre></td></tr></table><div class="operation"><span class="breakline-btn"><i class="ic i-align-left"></i></span><span class="copy-btn"><i class="ic i-clipboard"></i></span><span class="fullscreen-btn"><i class="ic i-expand"></i></span></div></div></figure><figure class="highlight java"><figcaption data-lang="java"></figcaption><div class="code-container"><table><tr><td data-num="1"></td><td><pre>package com.hadoop.demo.mapreduce;</pre></td></tr><tr><td data-num="2"></td><td><pre>import org.apache.hadoop.conf.Configuration;</pre></td></tr><tr><td data-num="3"></td><td><pre>import org.apache.hadoop.fs.Path;</pre></td></tr><tr><td data-num="4"></td><td><pre>import org.apache.hadoop.io.IntWritable;</pre></td></tr><tr><td data-num="5"></td><td><pre>import org.apache.hadoop.io.Text;</pre></td></tr><tr><td data-num="6"></td><td><pre>import org.apache.hadoop.mapreduce.Job;</pre></td></tr><tr><td data-num="7"></td><td><pre>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</pre></td></tr><tr><td data-num="8"></td><td><pre>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</pre></td></tr><tr><td data-num="9"></td><td><pre>import java.io.IOException;</pre></td></tr><tr><td data-num="10"></td><td><pre>public class MyJob {</pre></td></tr><tr><td data-num="11"></td><td><pre>    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {</pre></td></tr><tr><td data-num="12"></td><td><pre>        //1 Configuration创建配置对象</pre></td></tr><tr><td data-num="13"></td><td><pre>        Configuration cfg=new Configuration();</pre></td></tr><tr><td data-num="14"></td><td><pre>        //2 getInstance创建job对象</pre></td></tr><tr><td data-num="15"></td><td><pre>        Job job = Job.getInstance(cfg);</pre></td></tr><tr><td data-num="16"></td><td><pre>        //3 set设置job,map,reduce类,map,reduce输出类</pre></td></tr><tr><td data-num="17"></td><td><pre>        job.setJarByClass(MyJob.class);</pre></td></tr><tr><td data-num="18"></td><td><pre>        job.setMapperClass(MyMap.class);</pre></td></tr><tr><td data-num="19"></td><td><pre>        job.setReducerClass(MyReduce.class);</pre></td></tr><tr><td data-num="20"></td><td><pre>        job.setOutputKeyClass(Text.class);</pre></td></tr><tr><td data-num="21"></td><td><pre>        job.setOutputValueClass(IntWritable.class);</pre></td></tr><tr><td data-num="22"></td><td><pre>        job.setMapOutputKeyClass(Text.class);</pre></td></tr><tr><td data-num="23"></td><td><pre>        job.setMapOutputValueClass(IntWritable.class);</pre></td></tr><tr><td data-num="24"></td><td><pre>        //4 FileInputFormat.addInputPath指定读取地址</pre></td></tr><tr><td data-num="25"></td><td><pre>        FileInputFormat.addInputPath(job,new Path(&quot;hdfs://localhost:9000/wc/input&quot;));</pre></td></tr><tr><td data-num="26"></td><td><pre>        //5 FileOutputFormat.setOutputPath指定写入地址</pre></td></tr><tr><td data-num="27"></td><td><pre>        FileOutputFormat.setOutputPath(job,new Path(&quot;hdfs://localhost:9000/wc/output&quot;));</pre></td></tr><tr><td data-num="28"></td><td><pre>        //6 waitForCompletion等待mapreduce完成</pre></td></tr><tr><td data-num="29"></td><td><pre>        boolean flag = job.waitForCompletion(true);</pre></td></tr><tr><td data-num="30"></td><td><pre>        //7 System.exit关闭系统</pre></td></tr><tr><td data-num="31"></td><td><pre>        System.exit(flag?0:1);</pre></td></tr><tr><td data-num="32"></td><td><pre>    }</pre></td></tr><tr><td data-num="33"></td><td><pre>}</pre></td></tr></table><div class="operation"><span class="breakline-btn"><i class="ic i-align-left"></i></span><span class="copy-btn"><i class="ic i-clipboard"></i></span><span class="fullscreen-btn"><i class="ic i-expand"></i></span></div></div></figure><figure class="highlight java"><figcaption data-lang="java"></figcaption><div class="code-container"><table><tr><td data-num="1"></td><td><pre>package com.hadoop.demo.mapreduce;</pre></td></tr><tr><td data-num="2"></td><td><pre>import org.apache.hadoop.io.IntWritable;</pre></td></tr><tr><td data-num="3"></td><td><pre>import org.apache.hadoop.io.LongWritable;</pre></td></tr><tr><td data-num="4"></td><td><pre>import org.apache.hadoop.io.Text;</pre></td></tr><tr><td data-num="5"></td><td><pre>import org.apache.hadoop.mapreduce.Mapper;</pre></td></tr><tr><td data-num="6"></td><td><pre>import java.io.IOException;</pre></td></tr><tr><td data-num="7"></td><td><pre>public class MyMap extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt; {</pre></td></tr><tr><td data-num="8"></td><td><pre>    private IntWritable count=new IntWritable(1);</pre></td></tr><tr><td data-num="9"></td><td><pre>    private Text w=new Text();</pre></td></tr><tr><td data-num="10"></td><td><pre>    @Override</pre></td></tr><tr><td data-num="11"></td><td><pre>    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {</pre></td></tr><tr><td data-num="12"></td><td><pre>        //1 toString字符串转换</pre></td></tr><tr><td data-num="13"></td><td><pre>        String line = value.toString();</pre></td></tr><tr><td data-num="14"></td><td><pre>        //2 split分割单词</pre></td></tr><tr><td data-num="15"></td><td><pre>        String[] words = line.split(&quot; &quot;);</pre></td></tr><tr><td data-num="16"></td><td><pre>        //3 for遍历</pre></td></tr><tr><td data-num="17"></td><td><pre>        for (String word:words){</pre></td></tr><tr><td data-num="18"></td><td><pre>            //4 set转换Text</pre></td></tr><tr><td data-num="19"></td><td><pre>            w.set(word);</pre></td></tr><tr><td data-num="20"></td><td><pre>            //5 write写入</pre></td></tr><tr><td data-num="21"></td><td><pre>            context.write(w,count);</pre></td></tr><tr><td data-num="22"></td><td><pre>        }</pre></td></tr><tr><td data-num="23"></td><td><pre>    }</pre></td></tr><tr><td data-num="24"></td><td><pre>}</pre></td></tr></table><div class="operation"><span class="breakline-btn"><i class="ic i-align-left"></i></span><span class="copy-btn"><i class="ic i-clipboard"></i></span><span class="fullscreen-btn"><i class="ic i-expand"></i></span></div></div></figure><figure class="highlight java"><figcaption data-lang="java"></figcaption><div class="code-container"><table><tr><td data-num="1"></td><td><pre>package com.hadoop.demo.mapreduce;</pre></td></tr><tr><td data-num="2"></td><td><pre>import org.apache.hadoop.io.IntWritable;</pre></td></tr><tr><td data-num="3"></td><td><pre>import org.apache.hadoop.io.Text;</pre></td></tr><tr><td data-num="4"></td><td><pre>import org.apache.hadoop.mapreduce.Reducer;</pre></td></tr><tr><td data-num="5"></td><td><pre>import java.io.IOException;</pre></td></tr><tr><td data-num="6"></td><td><pre>public class MyReduce extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {</pre></td></tr><tr><td data-num="7"></td><td><pre>    private IntWritable i=new IntWritable();</pre></td></tr><tr><td data-num="8"></td><td><pre>    @Override</pre></td></tr><tr><td data-num="9"></td><td><pre>    protected void reduce(Text word, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {</pre></td></tr><tr><td data-num="10"></td><td><pre>        //1 int计数器</pre></td></tr><tr><td data-num="11"></td><td><pre>        int sum=0;</pre></td></tr><tr><td data-num="12"></td><td><pre>        //2 for遍历value</pre></td></tr><tr><td data-num="13"></td><td><pre>        for (IntWritable it:values){</pre></td></tr><tr><td data-num="14"></td><td><pre>            //3 get转换int</pre></td></tr><tr><td data-num="15"></td><td><pre>            sum +=it.get();</pre></td></tr><tr><td data-num="16"></td><td><pre>        }</pre></td></tr><tr><td data-num="17"></td><td><pre>        //4 set转换IntWritable</pre></td></tr><tr><td data-num="18"></td><td><pre>        i.set(sum);</pre></td></tr><tr><td data-num="19"></td><td><pre>        //5 write写入</pre></td></tr><tr><td data-num="20"></td><td><pre>        context.write(word,i);</pre></td></tr><tr><td data-num="21"></td><td><pre>    }</pre></td></tr><tr><td data-num="22"></td><td><pre>}</pre></td></tr></table><div class="operation"><span class="breakline-btn"><i class="ic i-align-left"></i></span><span class="copy-btn"><i class="ic i-clipboard"></i></span><span class="fullscreen-btn"><i class="ic i-expand"></i></span></div></div></figure><p>运行main方法就可以了</p><p><img src="https://img-blog.csdnimg.cn/20210423143111343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM2OTM3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="运行结果" tabindex="-1"><a class="header-anchor" href="#运行结果" aria-hidden="true">#</a> 运行结果</h2><p><img src="https://img-blog.csdnimg.cn/20210423142856778.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM2OTM3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210423142925259.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM2OTM3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="结语" tabindex="-1"><a class="header-anchor" href="#结语" aria-hidden="true">#</a> 结语</h1><p>个人感觉新版本对新手友好一下,HDFS WebUI 里可以上传文件,创建目录,2.7.1好像没有.</p>',58),n=[p];function i(o,s){return d(),a("div",null,n)}const c=t(e,[["render",i],["__file","15d9e331.html.vue"]]);export{c as default};
